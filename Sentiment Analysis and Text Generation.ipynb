{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sentiment Analysis\n",
    "Run sentiment analysis on a reddit-comment.csv file to filter each comment into a positive, negative, or neutral text file for text generation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "import pandas as pd\n",
    "\n",
    "# initializing model and tokenizer\n",
    "roberta = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(roberta)\n",
    "tokenizer = AutoTokenizer.from_pretrained(roberta)\n",
    "\n",
    "\n",
    "# preprocessessing step\n",
    "def read_csv(file_name):\n",
    "    # data from csv file is read and saved as a pandas dataframe\n",
    "    ds = pd.read_csv(file_name)\n",
    "\n",
    "    # drops unecessary columns\n",
    "    ds = ds.drop(columns=['bs1'])\n",
    "    ds = ds.drop(columns=['bs2'])\n",
    "    ds = ds.drop(columns=['bs3'])\n",
    "    ds = ds.drop(columns=['bs4'])\n",
    "    ds = ds.drop(columns=['bs5'])\n",
    "    ds = ds.drop(columns=['bs6'])\n",
    "    ds = ds.drop(columns=['bs7'])\n",
    "    ds = ds.drop(columns=['main thread'])\n",
    "    ds = ds.drop(columns=['Subreddit'])\n",
    "    ds = ds.drop(columns=['num'])\n",
    "    ds = ds.fillna(\" \")\n",
    "\n",
    "    return ds\n",
    "\n",
    "# runs given sentence through the roberta model to get sentiment scores\n",
    "def polarity_scores_roberta(sentence):\n",
    "    words = []\n",
    "    # filtering out nonsensical data\n",
    "    for word in sentence.split(' '):\n",
    "        if word.startswith('@') and len(word) > 1:\n",
    "            word = '@user'\n",
    "        elif word.startswith('http'):\n",
    "            word = 'http'\n",
    "        elif word.startswith('www'):\n",
    "            word = 'www'\n",
    "        words.append(word)\n",
    "    processed_data = ' '.join(words)\n",
    "\n",
    "    # tokenizing data and converting into a tensor for model\n",
    "    encoded_text = tokenizer(processed_data, return_tensors='pt')\n",
    "    # calling model on text\n",
    "    output = model(**encoded_text)\n",
    "    # saving the scores into a numpy array\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    # taking the softmax of the scores returned by the model\n",
    "    scores = softmax(scores)\n",
    "    scores_dict = {''\n",
    "        'negative': scores[0],\n",
    "        'neutral': scores[1],\n",
    "        'positive': scores[2]\n",
    "    }\n",
    "    # determining the main sentiment of the text and then filtering the text to the appropriate txt file\n",
    "    sentiment = max(scores[0], scores[1], scores[2])\n",
    "\n",
    "    if sentiment == scores[0]:\n",
    "        fr = open('negative_text.txt', 'a')\n",
    "        fr.write(processed_data + \"\\n\")\n",
    "        fr.close()\n",
    "    elif sentiment == scores[1]:\n",
    "        fr = open('neutral_text.txt', 'a')\n",
    "        fr.write(processed_data + \"\\n\")\n",
    "        fr.close()\n",
    "    else:\n",
    "        fr = open('positive_text.txt', 'a')\n",
    "        fr.write(processed_data + \"\\n\")\n",
    "        fr.close()\n",
    "\n",
    "    return scores_dict\n",
    "\n",
    "# iterates through processed pandas data frame and runs the model on the entire dataset\n",
    "def SentimentAnalysis_on_data(data):\n",
    "    res = {}\n",
    "    for i, j in data.itertuples(index=False):\n",
    "        text = i\n",
    "        myid = j\n",
    "        try:\n",
    "            result = polarity_scores_roberta(text)\n",
    "            res[myid] = {**result}\n",
    "         # sometimes the text can be too long for the roberta model yeilding in a runtime error\n",
    "         # model needs to continute running despite runtime errors\n",
    "        except RuntimeError:\n",
    "            print(\"Comment is too long\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # reads in csv file about comments on motorcycle subreddit\n",
    "    ds = read_csv('lifestyle_motorcycles.csv')\n",
    "    data = SentimentAnalysis_on_data(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CNN-LSTM and LSTM text generation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_metric\n",
    "\n",
    "# sets the device to use GPU in order to decrease runtime\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "bleu = load_metric(\"bleu\")\n",
    "\n",
    "# reads in file character by character and adds all characters into a list\n",
    "def read_data(file_name):\n",
    "    with open(file_name) as text_file:\n",
    "        text = text_file.read()\n",
    "\n",
    "    return text\n",
    "\n",
    "# creates an index to char dictionary and vice versa leading up to the encoding step\n",
    "def char_tokenization(text):\n",
    "    char_to_index = {}\n",
    "    index_to_char = {}\n",
    "    index = 0\n",
    "    for char in text:\n",
    "        if char not in char_to_index:\n",
    "            char_to_index[char] = index\n",
    "            index_to_char[index] = char\n",
    "            index += 1\n",
    "    return char_to_index, index_to_char\n",
    "\n",
    "# creates a tokenized vocabulary using the character to index dictionary\n",
    "# and converts tokenized text list into a tensor\n",
    "def convert_text_to_tokenized_tensor(text, char_to_index):\n",
    "    text_tensor = []\n",
    "    for char in text:\n",
    "        text_tensor.append(char_to_index[char])\n",
    "    text_tensor = torch.LongTensor(text_tensor).unsqueeze(dim=1)\n",
    "\n",
    "    # move tensor to GPU\n",
    "    text_tensor = text_tensor.to(device)\n",
    "    return text_tensor\n",
    "\n",
    "# define LSTM model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embedder = nn.Embedding(input_size, input_size)\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_sequence, hidden_state):\n",
    "        input_sequence = self.embedder(input_sequence)\n",
    "\n",
    "        # sends data to the LSTM layer\n",
    "        output, hidden_state = self.lstm(input_sequence, hidden_state)\n",
    "\n",
    "        # sends data to the fully connected linear layer\n",
    "        output = self.decoder(output)\n",
    "        return output, (hidden_state[0].detach(), hidden_state[1].detach())\n",
    "\n",
    "# define CNN LSTM model\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers, kernel_size):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        self.embedder = nn.Embedding(input_size, input_size)\n",
    "        self.cnn1d = nn.Conv1d(in_channels=input_size * 100, out_channels=input_size * 100, kernel_size=3, stride=1,\n",
    "                               padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=kernel_size, stride=1)\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_sequence, hidden_state):\n",
    "        input_sequence = self.embedder(input_sequence)\n",
    "\n",
    "        # reshapes word embedded input for CNN layer\n",
    "        input_sequence = torch.flatten(input_sequence)\n",
    "        unflatten_cnn = torch.nn.Unflatten(0, (7800, 1))\n",
    "        input_sequence = unflatten_cnn(input_sequence)\n",
    "\n",
    "        # sends data to the CNN layer\n",
    "        input_sequence = self.cnn1d(input_sequence)\n",
    "        input_sequence = torch.relu((input_sequence))\n",
    "        # reshapes tensor to appropriate shape for LSTM layer\n",
    "        unflatten_lstm = torch.nn.Unflatten(0, (100, 78))\n",
    "        input_sequence = unflatten_lstm(input_sequence)\n",
    "        input_sequence = input_sequence.reshape(100, 1, 78)\n",
    "\n",
    "        # sends data to the LSTM layer\n",
    "        output, hidden_state = self.lstm(input_sequence, hidden_state)\n",
    "        # sends data to the fully connected linear layer\n",
    "        output = self.decoder(output)\n",
    "        return output, (hidden_state[0].detach(), hidden_state[1].detach())\n",
    "\n",
    "# trains model over training data\n",
    "def train_model(char_to_index, num_of_epochs, input_sequence_len, text,\n",
    "                text_tensor, model):\n",
    "\n",
    "    # set model to use GPU\n",
    "    model = model.to(device)\n",
    "\n",
    "    # optimizer and loss function for backpropogation\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    training_loss_per_epoch = []\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(num_of_epochs):\n",
    "        print(f\"Epoch number: {epoch + 1}\\n\")\n",
    "\n",
    "        # initialize the hidden state and running loss\n",
    "        running_loss = 0\n",
    "        hidden_state = None\n",
    "\n",
    "        # random starting index\n",
    "        starting_index = range(torch.randint(high=input_sequence_len, size=(1,)).item(), len(text) - input_sequence_len,\n",
    "                               input_sequence_len)\n",
    "        for cur_index in starting_index:\n",
    "            # define what the predictor and response should be for training\n",
    "            predict = text_tensor[cur_index:cur_index + input_sequence_len]\n",
    "            response = text_tensor[cur_index + 1:cur_index + input_sequence_len + 1]\n",
    "\n",
    "            # train the model and compute the loss\n",
    "            prediction, hidden_state = model(predict, hidden_state)\n",
    "            loss = loss_function(torch.squeeze(prediction), torch.squeeze(response))\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            # backpropogation step to learn the weights\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        training_loss_per_epoch.append(running_loss / len(starting_index))\n",
    "\n",
    "    return model, training_loss_per_epoch\n",
    "\n",
    "# generates text using trained model\n",
    "def generate_text(model, training_loss_per_epoch,text_tensor, num_of_epochs, text, num_of_chars_to_generate, index_to_char):\n",
    "\n",
    "    # file for saving generated text\n",
    "    generated_text = open('generated_text_file.txt', 'a')\n",
    "\n",
    "    # number of epochs to generate text\n",
    "    for epoch in range(num_of_epochs):\n",
    "        print(f\"\\nEpoch number: {epoch + 1}\\n\")\n",
    "\n",
    "        #initializing the hidden state and the random seed input character\n",
    "        hidden_state = None\n",
    "        starting_rand_index = torch.randint(high=len(text) - 1, size=(1,)).item()\n",
    "        input_char = text_tensor[starting_rand_index:starting_rand_index + 1]\n",
    "\n",
    "        # print out the average training loss for the epoch\n",
    "        print(f\"Average Training Loss: {training_loss_per_epoch[epoch]}\\n\")\n",
    "\n",
    "        print(\"\\nGenerating Text:\\n\")\n",
    "        # generated text loop to generate 200 characters\n",
    "        for char in range(num_of_chars_to_generate):\n",
    "            # output of the model given input text and hiddent state\n",
    "            output, hidden_state = model(input_char, hidden_state)\n",
    "            # take the softmax to compute the probability distribution for next letter\n",
    "            # and sample from the distribution\n",
    "            probability = nn.functional.softmax(torch.squeeze(output), dim=0)\n",
    "            distribution = torch.distributions.Categorical(probability)\n",
    "            predicted_char = distribution.sample()\n",
    "            # print out the predicted char\n",
    "            print(index_to_char[predicted_char.item()], end='')\n",
    "\n",
    "            generated_text.write(index_to_char[predicted_char.item()] + \"\")\n",
    "            # set the next input for the model\n",
    "            input_char[0][0] = predicted_char.item()\n",
    "        generated_text.write(\"\\n\")\n",
    "    torch.save(model, \"generated_text.txt\")\n",
    "\n",
    "# graph the training loss\n",
    "def plot_training_loss(training_loss_per_epoch, num_of_epochs):\n",
    "    for i in range(num_of_epochs):\n",
    "        plt.plot(i, training_loss_per_epoch[i], 'o')\n",
    "\n",
    "    plt.title(\"Training Loss per Epoch graphed over 25 epochs for LSTM model\")\n",
    "    plt.xlabel(\"Number of Epochs\")\n",
    "    plt.ylabel(\"Training Loss Per Epoch\")\n",
    "    plt.show()\n",
    "\n",
    "def bleu_score_analysis(reference_file, prediction_file):\n",
    "  with open(reference_file) as reference_:\n",
    "        reference = reference_.read()\n",
    "\n",
    "  with open(prediction_file) as prediction_:\n",
    "        prediction = prediction_.read()\n",
    "\n",
    "  results = bleu.compute(predictions=prediction, references=reference)\n",
    "  print(results)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_of_hidden_layers = 512\n",
    "    num_of_layers = 3\n",
    "    kernel_size = 3\n",
    "\n",
    "\n",
    "    text = read_data(\"hamlet.txt\")\n",
    "\n",
    "    char_to_index, index_to_char = char_tokenization(text)\n",
    "\n",
    "    text_tensor = convert_text_to_tokenized_tensor(text, char_to_index)\n",
    "\n",
    "    # define the LSTM model\n",
    "    model = LSTM(input_size=len(char_to_index), output_size=len(char_to_index), hidden_size=num_of_hidden_layers,\n",
    "                      num_layers=num_of_layers)\n",
    "\n",
    "    # cnn_lstm_model = CNN_LSTM(input_size=len(char_to_index), output_size=len(char_to_index),\n",
    "    #                           hidden_size=num_of_hidden_layers, num_layers=num_of_layers,\n",
    "    #                           kernel_size=kernel_size)\n",
    "\n",
    "    trained_model, training_loss_per_epoch = train_model(char_to_index, num_of_epochs=25, input_sequence_len=100, text=text, text_tensor=text_tensor, model=model)\n",
    "\n",
    "    generate_text(model=trained_model, training_loss_per_epoch=training_loss_per_epoch,\n",
    "                  text_tensor=text_tensor, num_of_epochs=25, text=text,\n",
    "                  num_of_chars_to_generate=200, index_to_char=index_to_char)\n",
    "\n",
    "    plot_training_loss(training_loss_per_epoch=training_loss_per_epoch, num_of_epochs=25)\n",
    "    bleu_score_analysis(\"hamlet.txt\", \"generated_text_file.txt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
